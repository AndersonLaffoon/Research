<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Anderson's Research</title>
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
    <h1>Anderson Laffoon Research</h1>
        <h2>5 milestones</h2>
    <div class="ENIAC">
        <h3>ENIAC</h3>
            <img src="http://images.computerhistory.org/revonline/images/102618640-03-01.jpg?w=600" height="250px" width="400px">
            <p>Source: University of Pennsylvania Archives, computerhistory.org (<a href="https://www.computerhistory.org/revolution/birth-of-the-computer/4/78/317">https://www.computerhistory.org/revolution/birth-of-the-computer/4/78/317</a>)</p>
                <p><i>ENIAC</i>, or Electronic Numerical Integrator and Computer, was the first programmable general-use computer. It was built during World War II by the United States. At the Moore School of Electronic Engineering at the University of Pennsylvania, physicist John Mauchly, engineer J. Presper Eckert, Jr., and their colleagues led a project to build an all-electric computer. It was funded by the government. Work began in 1943 under contract to the army, under director Herman Goldstine. Within the next year, mathematician John von Neumann began to work with the group.</p>
                <p>It was designed for computing values for artillery range tables, but it lacked some of the characteristics that computers have today. It used plugboards for communication to the machine, which meant that once instructions were programmed, the machine ran at electronic speed. However, it took days to rewire the machine for each new task. Even though it took so long to program it, it was the most powerful calculating device built up to that point in time. Even though <i>ENIAC</i> was built for a specific purpose, it could execute different instructions or alter the execution of instructions based on the value of some data. This meant that it could be used for a wider range of problems.</p>
                <p><i>ENIAC</i> was very large. It took up a space of 50 ft by 30 ft in the basement of the Moore School. It had 40 U-shaped panels along three walls. Each panel was 2 ft by 2 ft by 8 ft. It had 17,000 vacuum tubes, 70,000 resistors, 10,000 capacitators, 6,000 switches, and 1,500 relays. It ran continuously, generating 174 kilowatts of heat, which meant that it needed its own air conditioning unit. It executed up to 5,000 additions per second, which was way faster than machines before.</p>
                    <p>Source: Paul A. Freiburger, britannica.com (<a href="https://www.britannica.com/technology/ENIAC">https://www.britannica.com/technology/ENIAC)</a></p>
    </div>
    <div class="IBM">
        <h3>IBM's Personal Computer</h3>
            <img src="https://www.ibm.com/ibm/history/ibm100/images/icp/S783702J66841T71/us__en_us__ibm100__ibm_pc__personal_computer__620x350.jpg" height="250px" width="400px">
            <p>Source: IBM, ibm.com (<a href="https://www.ibm.com/ibm/history/ibm100/images/icp/S783702J66841T71/us__en_us__ibm100__ibm_pc__personal_computer__620x350.jpg">https://www.ibm.com/ibm/history/ibm100/images/icp/S783702J66841T71/us__en_us__ibm100__ibm_pc__personal_computer__620x350.jpg)</a></p>
                <p>Personal computers were available to the public in even as early as the mid-1970’s; some were do-it-yourself kits, and others were off-the-shelf products. They had a few applications, but none were general enough for widespread use. In September 1975, IBM’s General Systems Division announced the IBM 5100 Portable Computer. It weighed 50 pounds and was comparable to the IBM 1130 in storage capacity and performance, but almost as small as an IBM Selectric Typewriter.</p>
                <p><i>IBM’s Personal Computer</i> (IBM 5150) was introduced in August 1981, a year after corporate executives gave Bill Lowe permission to go ahead with the project. Bill was the lab director in IBM’s Boca Raton, Florida facilities. He created a task force that came up with the proposal for the first <i>IBM PC</i>. He picked 12 strategists who worked diligently to create a plan for hardware, software, manufacturing setup, and sales strategy. Don Estridge, the acting lab director at the time, headed up the project. Joe Bauman was in charge of manufacturing and Mel Hallerman was brought in as chief programmer. After a year, at a press conference in New York City, Estridge announced that the <i>IMB Personal Computer</i> would cost $1,565. Just two decades earlier, IBM computers cost up to $9 million and required their own air-conditioning unit.</p>
                <p>The response to the announcement was insane. By the end of 1982, qualified retail outfits were signing on to sell the new machine at the rate of one-a-day as sales actually hit a system-a-minute every business day. Overall, <i>IBM’s Personal Computer</i> was an overwhelming success and proved to be a huge upgrade from computers in the past. </p>
                    <p>Source: ibm.com (<a href="https://www.ibm.com/ibm/history/exhibits/pc25/pc25_birth.html">https://www.ibm.com/ibm/history/exhibits/pc25/pc25_birth.html)</a></p>
    </div>
    <div class="WWW">
        <h3>The World Wide Web</h3>
            <img src="https://cdn.britannica.com/73/78373-050-2D15D41C/Tim-Berners-Lee.jpg" height="250px" width="400px">
            <p>Source: Sir Tim Berners-Lee, brittanica.com (<a href="https://www.britannica.com/topic/World-Wide-Web">https://www.britannica.com/topic/World-Wide-Web)</a></p>
                <p><i>The World Wide Web</i> is the leading information retrieval system of the Internet. It gives its users access to an incredible number of documents that are interconnected by hypertext or hypermedia links, which are electronic connections that link information in order to give access to users. <i>The World Wide Web</i> operates within the Internet’s client-server format; servers are computer programs that store and transmit documents to other computers on the network when asked to, while clients are programs that request documents from a server as the user asks for them.</p>
                <p>The creation of the <i>World Wide Web</i> started in 1989, by Tim Berners-Lee and his colleagues at CERN, which was an international scientific organization in Geneva, Switzerland. They created the Hypertext Transfer Protocol (HTML), which standardized communication between servers and clients. The text-based Web browser they created was made available to the public in January 1992.</p>
                <p><i>The World Wide Web</i> was made popular through the creation of the web browser Mosaic, a U.S. product created by Marc Andreessen and others in the National Center for Supercomputing Applications at the University of Illinois and was released in September 1993. Andreessen then founded Netscape Communications Corporation in April of 1994. Their Netscape Navigator then became the most popular Web browser soon after it was released in December 1994. By the mid-1990s, the <i>World Wide Web</i> had millions of active users. In 1995, Internet Explorer, created by Microsoft, was added to the Windows 95 operating system. As it was bundled with Microsoft, it quickly became the most popular web browser. As of current day, cell phone usage has led to increased web usage. Smartphones now account for more than half of Web browsing.</p>
    </div>
    <div class="Broadband">
        <h3>Broadband Internet</h3>
            <img src="https://i.insider.com/60674959a7d803001963e7b1?width=700" height="250px" width="400px">
            <p>Source: Business Insider, businessinsider.com (<a href="https://www.businessinsider.com/what-is-broadband-internet">https://www.businessinsider.com/what-is-broadband-internet)</a></p>
                <p><i>Broadband Internet</i> is, simply put, any high-speed internet service. It has been the most common kind of internet service in the United States for a couple of decades. Prior to the availability of <i>Broadband</i>, most people used dial-up internet service – also used for telephone calls. Dial-up service meant that picking up the phone would turn off all other internet access, and the internet’s speed was very slow (around 0.056 megabits per second). In current day, almost ever U.S. home uses <i>Broadband</i>. As opposed to dial-up internet, <i>Broadband Internet</i> has an average speed of 124 megabits per second, which is around 2,200 times faster. <i>Broadband</i> is essentially the type and speed of Wi-Fi signals, which are delivered to homes and offices through a router. The four major types of <i>Broadband Internet</i> are cable, DSL, fiber, and Satellite.</p>
                <p><i>Broadband</i> started to replace dial-up in the early 2000s, leading to half of all Internet users having a <i>Broadband</i> connection by 2007. <i>Broadband</i> is always connected to the internet, so those that switched didn’t need to be switched on and off all the time to work. Wireless Internet became commercially available to the public in 1999, with the release of the Apple Airport, followed by the release of a Windows Wi-Fi router in 2001. Wireless Internet hotspots were soon introduced to many businesses, which provided public access to the Internet, sometimes for free, and operate using a wireless local area network connected to a router. Overall, <i>Broadband Internet</i> greatly improved the speed, availability, and use of the internet.</p>
    </div>
    <div class="AI">
        <h3>Artificial Intelligence</h3>
            <img src="https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fspecials-images.forbesimg.com%2Fimageserve%2F5f278facccb6d3d3cc3c63b1%2F3-Important-Ways-Artificial-Intelligence-Will-Transform-Your-Business-And-Turbocharge%2F960x0.jpg%3Ffit%3Dscale" height="250px" width="400px">
            <p>Source: Bernard Marr, forbes.com (<a href="https://www.forbes.com/sites/bernardmarr/2020/08/03/3-important-ways-artificial-intelligence-will-transform-your-business-and-turbocharge-success/?sh=221c7dae620f">https://www.forbes.com/sites/bernardmarr/2020/08/03/3-important-ways-artificial-intelligence-will-transform-your-business-and-turbocharge-success/?sh=221c7dae620f)</a></p>
                <p><i>Artificial Intelligence</i> is the simulation of human intelligence processes by machines, especially computer systems. Often, what is referred to <i>AI</i> is just one component of it, such as machine learning. It requires a foundation of specialized hardware and software for writing and training machine learning algorithms. Python, R, and Java are popular programming languages that are popular with <i>AI</i>. <i>AI</i> works by ingesting large amounts of training data, analyzing the data for correlations and patterns, and using said patterns to make predictions about future states. <i>AI</i> consists of three major processes. The learning process of <i>AI</i> focuses on acquiring data and creating rules for how to turn the data into actionable information. Algorithms provide computing devices with step-by-step instructions for how to complete a specific task. The reasoning process focuses on choosing the right algorithm for a desired outcome. The self-correction process is designed to continually fine-tune algorithms and ensure they provide accurate results. Reactive machines, limited memory systems, theory of mind systems, and self-aware systems are examples of <i>AI</i>.</p>
                    <p>Source: Ed Burns, searchenterprise.techtarget.com (<a href=”https://searchenterpriseai.techtarget.com/definition/AI-Artificial-Intelligence#:~:text=Artificial%20intelligence%20is%20the%20simulation,speech%20recognition%20and%20machine%20vision.”> https://searchenterpriseai.techtarget.com/definition/AI-Artificial-Intelligence#:~:text=Artificial%20intelligence%20is%20the%20simulation,speech%20recognition%20and%20machine%20vision.)</a>)</p>
                <p>Alan Turing was a young British polymath who explored the mathematical possibility of <i>Artificial Intelligence</i>. He suggested that humans use available information as well as reason in order to solve problems, and that machines could possibly do the same. Unfortunately, Turing could not immediately begin work on his theory. In the 1950s, computers cost up to $200,000 a month to lease. Thus, Turing could not begin his work. The first <i>AI</i> program is considered by many to be Allen Newell, Cliff Shaw, and Herbert Simon’s Logic Theorist. It was previewed at the Dartmouth Summer Research Project on Artificial Intelligence in 1956. After this conference, it was widely accepted that <i>AI</i> was achievable.</p>
                <p>From 1957 to 1974, several advancements were made for <i>AI</i>. However, in the 70s, the hype for <i>AI</i> lost steam, and many people’s patience dwindled. In the 80s, <i>AI</i> research was reignited by a boost of funds. During the 90s and 2000s, most of the landmark goals of <i>AI</i> were achieved that we are familiar with today.</p>
                <p>Source: Rockwell Anyoha, harvard.edu (<a href=”https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/”>https://sitn.hms.harvard.edu/flash/2017/history-artificial-intelligence/)</a></p>
    </div>
    <table border="3">
         <tr>
            <td>ENIAC</td>
            <td>IBM's Personal Computer</td>
            <td>The World Wide Web</td>
            <td>Broadband Internet</td>
            <td>Artificial Intelligence</td>
        </tr>
        <tr>
            <td>2/15/1946</td>
            <td>8/12/1981</td>
            <td>1989</td>
            <td>1991</td>
            <td>1956</td>
    </table>
</body>
</html>